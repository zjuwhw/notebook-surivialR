[["index.html", "Preface", " Preface This is a notebook for reading the book “Applied Survival Analysis using R” "],["introduction.html", "1 Introduction 1.1 What is Survival Analysis 1.2 What you need to know to use this book 1.3 Survival Data and Censoring 1.4 Some examples of survival data sets", " 1 Introduction 1.1 What is Survival Analysis Survival analysis, survival times, factors influencing survival times 1.2 What you need to know to use this book 1.3 Survival Data and Censoring response variable, time-to-event, is a non-negative discrete or continuous random variable censoring, right censoring, results when the final endpoint is only known to exceed a particular value \\(T^*\\) is a random variable representing the time to failure U is a random variable representing the time to a censoring event what we observe is \\(T=min(T^*,U)\\) and a censoring indicator \\(\\delta=I[T^*&lt;U]\\). \\(\\delta\\) is 0 when T is a censored time, or 1 when T is observed. right-censoring, left-censoring (less common) Three types: - Type I, the censoring times are pre-specified - Type II, follow-up stops when a pre-specified fraction have failed - Random. Patient dropout. Competing events. Administrative censoring. Example 1.1. clinial trail; accural period + follow-up period administrative censoring; patient dropout: non-informative (unrelated) vs informative (related to the failure process) The goals of surivival analysis estimate the survival distribution compare two or more survival distributions assess the effects of a number of factors on survival 1.4 Some examples of survival data sets Example 1.2. Xelox (chemotherapy drug) in patients with advanced gastric (胃) cancer Progression-free survival: the time from entry into the clinical trial until progression or death, whichever comes first Wang et al. Cancer Chemother. Pharmacol 2014 #install.packages(&quot;asaur&quot;) library(asaur) gastricXelox[23:27,] ## timeWeeks delta ## 23 42 1 ## 24 43 1 ## 25 43 0 ## 26 46 1 ## 27 48 0 Example 1.3 Pancreatic cancer (胰腺癌) in patients with locally advanced or metastatic disease Moss et al., Gastrointest. Cancer Res. 2012 head(pancreatic) ## stage onstudy progression death ## 1 M 12/16/2005 2/2/2006 10/19/2006 ## 2 M 1/6/2006 2/26/2006 4/19/2006 ## 3 LA 2/3/2006 8/2/2006 1/19/2007 ## 4 M 3/30/2006 . 5/11/2006 ## 5 LA 4/27/2006 3/11/2007 5/29/2007 ## 6 M 5/7/2006 6/25/2006 10/11/2006 ### stage column: M means metastatic; LA means locally advanced Example 1.4 survival prospects of prostate cancer patients (前列腺癌) with high-risk disease Lu-Yao et al., J. Am. Med. Assoc, 2009 prostateSurvival[88:95,] ## grade stage ageGroup survTime status ## 88 poor T2 75-79 33 0 ## 89 mode T2 75-79 6 0 ## 90 mode T1c 75-79 15 2 ## 91 mode T2 70-74 6 2 ## 92 mode T1ab 80+ 93 1 ## 93 poor T2 80+ 60 2 ## 94 mode T2 80+ 1 0 ## 95 mode T1ab 75-79 34 0 ### grade: poorly or moderately differentiated ### cancer stage: T1c if screen-diagnosed using a prostate-specific antigen blood test, T1ab if clinically diagnosed without screening, or T2 if palpable at diagnosis ### surival time: days from diagnosis to death or date last seen ### status: 1 if died of prostate cancer, 2 if died of some other cause, or 0 still alive at the date last seen Example 1.5 Comparison of medical therapies to aid smokers to quit triple-mdication combination vs therapy with the nicotine patch alone time from randomization until relapse (return to smoking); censored at six months Steinberg Ann. Intern. Med 2009 pharmacoSmoking[1:6, 2:8] ## ttr relapse grp age gender race employment ## 1 182 0 patchOnly 36 Male white ft ## 2 14 1 patchOnly 41 Male white other ## 3 5 1 combination 25 Female white other ## 4 16 1 combination 54 Male white ft ## 5 0 1 combination 45 Male white other ## 6 182 0 combination 43 Male hispanic ft # ttr: time to relapse/the number of days without smoking # employment: ft (full time), pt (part time), or other Example 1.6 Prediction of survival of hepatocellular carcinoma (肝细胞癌) patients using biomarkers hepatoCellular[c(1,2,3,65,71),c(2,3,16:20,24,47)] ## Age Gender OS Death RFS Recurrence CXCL17T CD4N Ki67 ## 1 57 0 83 0 13 1 113.94724 0 6.04350 ## 2 58 1 81 0 81 0 54.07154 NA NA ## 3 65 1 79 0 79 0 22.18883 NA NA ## 65 38 1 5 1 5 1 106.78169 0 44.24411 ## 71 57 1 11 1 11 1 98.49680 0 99.59232 # OS: overall survival # RFS: recurrence-free survival "],["basic-principles-of-survival-analysis.html", "2 Basic Principles of Survival Analysis 2.1 Hazard and Survival Functions 2.2 Other Representations of a Survival Distribution 2.3 Mean and Median Survival Time 2.4 Parametric Survival Distributions 2.5 Computing the Survival Function from the Hazard Function 2.6 A breif introduction to maximum likelihood estimation", " 2 Basic Principles of Survival Analysis 2.1 Hazard and Survival Functions Survival function defines the probability of surviving up to a point t, \\[S(t)=pr(T&gt;t), 0&lt;t&lt;\\infty\\] , which decreases (or remains constant) over time Hazard function is the instantaneous (瞬间) failure rate, \\[h(t) = \\lim\\limits_{\\delta \\to \\infty}\\frac{pr(t&lt;T&lt;t+\\delta|T&gt;t)}{\\delta}\\] This function is also known as intensity function or the force of mortality Example 2.1 The daily hazard rates of men and women by age in each calendar year from 1940 to 2004 library(survival) tm &lt;- c(0, 1/365, # first day of life 7/365, # seventh day of life 28/365, # fourth week of life 1:110) # subsequent years hazMale &lt;- survexp.us[,&quot;male&quot;,&quot;2004&quot;] hazFemale &lt;- survexp.us[,&quot;female&quot;,&quot;2004&quot;] ## not sure how to get the harzard for the days or weeks in the first year plot(1:110, log10(hazMale), col=&quot;blue&quot;, type=&quot;l&quot;, ylab=&quot;log10(hazard)&quot;, xlab=&quot;age in years&quot;) lines(1:110, log10(hazFemale), col=&quot;red&quot;) legend(0, -2.7, legend=c(&quot;male&quot;, &quot;female&quot;), lty=1, col=c(&quot;blue&quot;, &quot;red&quot;)) 2.2 Other Representations of a Survival Distribution Cumulative distribution function (CDF), also known as the cumulative risk function \\[F(t) = pr(T \\le t) = 1-S(t), 0&lt;t&lt;\\infty\\] Probability density function (PDF), \\[f(t) = \\frac{d}{dt} F(t) = \\frac{d}{dt}[1-S(t)]=-\\frac{d}{dt}S(t)\\] , related to hazard function and survival functions \\[h(t)=\\frac{f(t)}{S(t)}\\] That is, the harzard at time t is the probability that an event occurs in the neighborhood of time t divided by the probability that the subject is alive at time t. The cumulative hazard function is defined as the area under the hazard function up to time t, \\[H(t)=\\int_0^th(u)du\\] The survival function may be defined in terms of the harzard function by \\[S(t)=exp(-\\int_0^th(u)du)=exp(-H(t))\\] Derivations: \\(H(t) = \\int_0^th(u)du\\) \\(=\\int_0^t\\frac{f(u)}{S(u)}du\\) \\(=\\int_0^t\\frac{-\\frac{d}{du}S(u)}{S(u)}du\\) \\(=\\int_0^t\\frac{-d(S(u))}{S(u)}\\) \\(=-ln(S(u))|^t_0\\) \\(=-ln(S(t))+ln(S(0))\\) \\(=-ln(S(t))\\) 2.3 Mean and Median Survival Time \\[\\mu=E(T)=\\int_0^\\infty tf(t)dt\\] Based on \\(\\lim\\limits_{t \\to \\infty}(tS(t))=0\\), we can have a formula \\[\\mu=\\int_0^\\infty S(t)dt\\] The mean survival time is only defined if \\(S(\\infty)=0\\) (such as time to death), but not in the case \\(S(\\infty)=c\\) (such as cancer recurrence; fraction c of subjects will not recurrent, and the area under the survival curve is infinite) The median survival time is defined as the time t such that \\(S(t)=1/2\\). 2.4 Parametric Survival Distributions The exponential distribution, the simplest survival distribution, has a constant hazard, \\(h(t)=\\lambda\\) The cumulative hazard function \\[H(t)=\\int_0^t h(u)du=\\int_0^t \\lambda du=\\lambda t|_0^t=\\lambda t\\] The survival function is \\[S(t)=e^{-H(t)}=e^{-\\lambda t}\\] The probability density function is \\[f(t) = h(t)S(t)=\\lambda e^{-\\lambda t}\\] The mean of survival time is \\[E(T)=\\int_0^{\\infty} S(t)dt=\\int_0^{\\infty} e^{-\\lambda t}dt = 1/\\lambda\\] The median of survival time is \\[0.5=e^{-\\lambda t}\\text{, so }t_{med}=ln(2)/\\lambda\\] lambda=1/70 layout(matrix(1:4,2,2)) plot(1:100, rep(lambda,100), xlab=&quot;time&quot;, ylab=&quot;Hazard&quot;, ylim=c(0,2/70), type=&quot;l&quot;) plot(1:100, exp(-lambda*(1:100)), xlab=&quot;time&quot;, ylab=&quot;Survival&quot;, type=&quot;l&quot;) plot(1:100, lambda*(1:100), xlab=&quot;time&quot;, ylab=&quot;Cumulative hazard&quot;, type=&quot;l&quot;) plot(1:100, lambda*exp(-lambda*(1:100)), xlab=&quot;time&quot;, ylab=&quot;PDF&quot;, type=&quot;l&quot;) Another distibution is Weibull distribution. Its hazard function \\[h(t)=\\alpha\\lambda(\\lambda t)^{\\alpha-1}=\\alpha\\lambda^{\\alpha}t^{\\alpha-1}\\] The cumulative hazard \\[H(t)=(\\lambda t)^{\\alpha}\\] The survival function \\[S(t) = e^{-(\\lambda t)^{\\alpha}}\\] The mean of survival time \\[E(T)=\\frac{\\Gamma(1+1/\\alpha)}{\\lambda}\\] , where \\(\\Gamma(x)=(x-1)!\\) is gamma function The median of survival time \\[t_{med}=\\frac{[ln(2)]^{1/\\alpha}}{\\lambda}\\] The exponential distribution is a special case with \\(\\alpha=1\\) h = function(t, alpha, lambda){ return(alpha*(lambda^alpha)*(t^(alpha-1))) } lambda=0.03 plot(1:80, h(1:80, alpha=1, lambda), xlab=&quot;time&quot;, ylab=&quot;hazard&quot;, type=&quot;l&quot;, ylim=c(0,0.08)) lines(1:80, h(1:80, alpha=1.5, lambda), col=&quot;red&quot;) lines(1:80, h(1:80, alpha=0.75, lambda), col=&quot;blue&quot;) text(40, 0.035, &quot;alpha=1, lambda=0.03&quot;) text(40, 0.06, &quot;alpha=1.5, lambda=0.03&quot;, col=&quot;red&quot;) text(40, 0.01, &quot;alpha=0.75, lambda=0.03&quot;, col=&quot;blue&quot;) ### Weibull distribution can be calculated dweilbull() or pweibull() h2 = function(t, alpha, lambda){ return(dweibull(t, shape=alpha, scale=1/lambda)/pweibull(t, shape=alpha, scale=1/lambda, lower.tail=F)) } plot(1:80, h2(1:80, alpha=1, lambda), xlab=&quot;time&quot;, ylab=&quot;hazard&quot;, type=&quot;l&quot;, ylim=c(0,0.08), main=&quot;using dweibull and pweibull function&quot;) lines(1:80, h2(1:80, alpha=1.5, lambda), col=&quot;red&quot;) lines(1:80, h2(1:80, alpha=0.75, lambda), col=&quot;blue&quot;) text(40, 0.035, &quot;alpha=1, lambda=0.03&quot;) text(40, 0.06, &quot;alpha=1.5, lambda=0.03&quot;, col=&quot;red&quot;) text(40, 0.01, &quot;alpha=0.75, lambda=0.03&quot;, col=&quot;blue&quot;) ### generate random variables set.seed(1) tt.weib = rweibull(1000, shape=1.5, scale=1/0.03) mean(tt.weib); median(tt.weib) ## [1] 29.90708 ## [1] 26.95561 ### the theoretical mean and median gamma(1+1/1.5)/0.03 ## [1] 30.09151 (log(2)^(1/1.5))/0.03 ## [1] 26.10733 A third distribution can be gamma distribution, with the PDF \\[f(t)=\\frac{\\lambda^\\beta t^{\\beta-1}exp[-\\lambda t]}{\\Gamma(\\beta)}\\] Its hazard function cannot be written in closed form, but can be computed using R gammaHaz = function(t, shape, scale){ return(dgamma(t, shape=shape, scale=scale)/pgamma(t, shape=shape, scale=scale, lower.tail=F)) } There are other parametric families of survival distributions, including log-normal, log-logistic, Pareto, and many more. 2.5 Computing the Survival Function from the Hazard Function For complicated hazard function, we can use numerical methods to evaluate the integral. ### back to the Example 2.1 library(survival) tm = 0:110 hazMale = survexp.us[,&quot;male&quot;, &quot;2004&quot;] hazFemale = survexp.us[,&quot;female&quot;, &quot;2004&quot;] tm.diff = diff(tm) ## time intervals ## survival survMale = exp(-cumsum(hazMale*tm.diff)*365.24) survFemale = exp(-cumsum(hazFemale*tm.diff)*365.24) plot(1:110, survMale, col=&quot;blue&quot;, type=&quot;l&quot;, ylab=&quot;Survival&quot;, xlab=&quot;age in years&quot;) lines(1:110, survFemale, col=&quot;red&quot;) legend(0, 0.2, legend=c(&quot;male&quot;, &quot;female&quot;), lty=1, col=c(&quot;blue&quot;, &quot;red&quot;)) sum(survMale*tm.diff) ## [1] 74.38014 sum(survFemale*tm.diff) ## [1] 79.44648 2.6 A breif introduction to maximum likelihood estimation Suppose that we have a series of observations \\(t_1, t_2, ..., t_n\\) from an exponential distribution with unknown parameter \\(\\lambda\\). Maximum likelihood estimation can provides a mathematical framework to estimate \\(\\lambda\\) The likelihood function is \\[L(\\lambda; t_1, t_2, ..., t_n)=f(t_1, \\lambda)f(t_2, \\lambda)...f(t_n,\\lambda)=\\prod^n_{i=1}f(t_i,\\lambda)\\] Considering right-censored observation, we put in the survival function, indicating that obersvation is known only to exceed a particular value. \\[L(\\lambda; t_1, t_2, ..., t_n)=\\prod_{i=1}^n f(t_i, \\lambda)^{\\delta_i}S(t_i, \\lambda)^{1-\\delta_i}=\\prod_{i=1}^n h(t_i, \\lambda)^{\\delta_i}S(t_i,\\lambda)\\] , where \\(\\delta_i\\) indicates the censoring status, that is \\(\\delta_i=1\\) for an observed data, and \\(\\delta_i=0\\) for a censored data For the exponential distribution, \\[L(\\lambda)=\\prod_{i=1}^n[\\lambda e^{-t_i\\lambda}]^{\\delta_i}[e^{-\\lambda t_i}]^{1-\\delta_i}=\\prod_{i=1}^n[\\lambda^{\\delta_i}e^{-\\lambda t_i}]\\] Set the total number of deaths as \\(d=\\sum_{i=1}^n\\delta_i\\), and the total amount of time of patients on the study (called as person-years in epidemiology) as \\(V=\\sum_{i=1}^n t_i\\), then the likelihood function \\[L(\\lambda)=\\lambda^d e^{-\\lambda V}\\] The log-likelihood function \\[l(\\lambda) = d*log(\\lambda) - \\lambda V\\] The first derivative (also called the score function), \\[l&#39;(\\lambda)=\\frac{d}{\\lambda}-V\\] Set \\(l&#39;(\\lambda)=0\\), we can get the maximum likelihood estimate (MLE), \\[\\hat{\\lambda} = d/V\\] The second derivative \\[l&#39;&#39;(\\lambda) = -\\frac{d}{\\lambda^2}=-I(\\lambda)\\] , where \\(I(\\lambda)\\) is known as information function, where its inverse is approximately the variance of the MLE \\[var(\\hat{\\lambda})\\approx I^{-1}(\\lambda)=\\lambda^2/d\\] In practice, we can use \\(\\hat{\\lambda}\\) to replace \\(\\lambda\\) \\[var(\\hat{\\lambda})\\approx I^{-1}(\\lambda)=\\hat{\\lambda}^2/d=d/V^2\\] Back to the example 1.1, \\(d=3\\), and \\(V=7+6+6+5+2+4=30\\), so \\(\\hat{\\lambda}=3/30=0.1\\) and the log-likelihood function is \\(l(\\lambda)=d log(\\lambda)-\\lambda V=3*log(0.1)-0.1*30=-9.9\\) when MLE library(survival) time = c(7,6,6,5,2,4) status = c(0,1,0,0,1,1) surv_obj &lt;- Surv(time, status) fit &lt;- survreg(surv_obj ~ 1, dist = &quot;exponential&quot;) summary(fit) ## ## Call: ## survreg(formula = surv_obj ~ 1, dist = &quot;exponential&quot;) ## Value Std. Error z p ## (Intercept) 2.303 0.577 3.99 6.7e-05 ## ## Scale fixed at 1 ## ## Exponential distribution ## Loglik(model)= -9.9 Loglik(intercept only)= -9.9 ## Number of Newton-Raphson Iterations: 4 ## n= 6 ### lambda ### the coefficients of survreg() is log(1/theta), where theta is called ### scale parameter and is 1/lambda 1/exp(fit$coefficients) ## (Intercept) ## 0.1 ### theory d=sum(status); V=sum(time) d/V ### lambda ## [1] 0.1 "],["nonparametric-survival-curve-estimation.html", "3 Nonparametric Survival Curve Estimation 3.1 Nonparametric Estimation of the Survival Function 3.2 Finding the median survival and a confidence interval for the median 3.3 Median Follow-up Time 3.4 Obtaining a smoothed Hazard and Survival Function estimate 3.5 Left Truncation", " 3 Nonparametric Survival Curve Estimation 3.1 Nonparametric Estimation of the Survival Function product-limit estimator/Kaplan-Meier estimator (Kaplan and Meier, 1958): the product over the failure times of the conditional probabilities of surviving to the next failure time, \\[\\hat{S}(t)=\\prod_{t_i \\le t}(1-\\hat{q}_i) = \\prod_{t_i \\le t}(1-\\frac{d_i}{n_i})\\] , where \\(n_i\\) is the number of subjects at risk at time \\(t_i\\) \\(d_i\\) is the number of individuals who fail at that time. Back to example 1.1 The median survival time is at t=6, which is the smallest time t such that S(t)&lt;=0.5. Based on “delta method”, the sampling variance of \\(log(\\hat{S}(t))\\), \\[var(log \\hat{S}(t))=\\sum_{t_i \\le t}var(log(1-\\hat{q}_i))\\approx \\sum_{t_i \\le t}\\frac{d_i}{n_i(n_i-d_i)}\\] Use the delta method again, \\[var(\\hat{S}(t)) \\approx [\\hat{S}(t)]^2\\sum_{t_i \\le t}\\frac{d_i}{n_i(n_i-d_i)}\\] Another (more satisfying) approach is via the complementary log-log transformation of \\(\\hat{S}(t)\\) \\[var(log[-log \\hat{S}(t)]) \\approx \\frac{1}{[log \\hat{S}(t)]^2}\\sum_{t_i \\le t}\\frac{d_i}{n_i(n_i-d_i)}\\] library(survival) time = c(7,6,6,5,2,4) status=c(0,1,0,0,1,1) Surv(time, status) ## [1] 7+ 6 6+ 5+ 2 4 result.km = survfit(Surv(time, status)~1) result.km ## Call: survfit(formula = Surv(time, status) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## [1,] 6 3 6 4 NA summary(result.km) ## Call: survfit(formula = Surv(time, status) ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.833 0.152 0.583 1 ## 4 5 1 0.667 0.192 0.379 1 ## 6 3 1 0.444 0.222 0.167 1 ## the default value for conf.type option is &quot;log&quot; result.km2 = survfit(Surv(time, status)~1, conf.type=&quot;log-log&quot;) summary(result.km2) ## Call: survfit(formula = Surv(time, status) ~ 1, conf.type = &quot;log-log&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.833 0.152 0.2731 0.975 ## 4 5 1 0.667 0.192 0.1946 0.904 ## 6 3 1 0.444 0.222 0.0662 0.785 layout(matrix(1:2,1,2)) plot(result.km, main=&quot;log&quot;) plot(result.km2, main=&quot;log-log&quot;) An alternative estimator of the survival function is Nelson-Aalen estimator/Fleming and Harrington, based on the relationship of the survival function to the hazard function \\[S(t)=e^{-H(t)}\\text{ and } H(t)=\\sum_{t_i \\le t}\\frac{d_i}{n_i}\\] result.fh = survfit(Surv(time, status)~1, conf.type=&quot;log-log&quot;, type=&quot;fh&quot;) summary(result.fh) ## Call: survfit(formula = Surv(time, status) ~ 1, conf.type = &quot;log-log&quot;, ## type = &quot;fh&quot;) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 6 1 0.846 0.141 0.306 0.977 ## 4 5 1 0.693 0.180 0.229 0.913 ## 6 3 1 0.497 0.210 0.101 0.807 Back to example 1.2 library(asaur) timeMonths = gastricXelox$timeWeeks * 7 /30.25 delta = gastricXelox$delta result.km = survfit(Surv(timeMonths, delta)~1, conf.type=&quot;log-log&quot;) plot(result.km, conf.int=T, mark=&quot;|&quot;, xlab=&quot;Time in months&quot;, ylab=&quot;Survival probability&quot;, main=&quot;Progression-free survival in gastric cancer patients&quot;) 3.2 Finding the median survival and a confidence interval for the median Formally, the median survival time is defined as \\(\\hat{t}_{med}=inf\\{t: \\hat{S}(t) \\le 0.5\\}\\). That is the smallest t such that the survival function is less than or equal to 0.5. To find a \\(1-\\alpha\\) confidence interval for the median, we consider the following inequality: \\[-z_{\\alpha/2}\\le \\frac{g(\\hat{S}(t))-g(0.5)}{\\sqrt{var[g(\\hat{S}(t))]}} \\le z_{\\alpha/2}\\] , where \\(g(u)=log[-log(u)]\\) and \\(var[g(\\hat{S}(t))]\\) has been given above result.km ## Call: survfit(formula = Surv(timeMonths, delta) ~ 1, conf.type = &quot;log-log&quot;) ## ## n events median 0.95LCL 0.95UCL ## [1,] 48 32 10.3 5.79 15.3 plot(result.km) abline(h=0.5, col=&quot;red&quot;, lty=2) segments(x0 = 10.3, y0 = -1, x1 = 10.3, y1 = 0.5, col = &quot;green&quot;, lty = 2) segments(x0 = 5.79, y0 = -1, x1 = 5.79, y1 = 0.5, col = &quot;blue&quot;, lty = 2) segments(x0 = 15.3, y0 = -1, x1 = 15.3, y1 = 0.5, col = &quot;blue&quot;, lty = 2) 3.3 Median Follow-up Time A simple definition is to consider all of the survival times A perhaps better way is to first switch the censoring and death indicators, and then computes the KM estimate. This method is also known as the “reverse” Kaplan-Meier. delta.followup = 1-delta survfit(Surv(timeMonths, delta.followup)~1) ## Call: survfit(formula = Surv(timeMonths, delta.followup) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## [1,] 48 16 27.8 21.1 50.2 3.4 Obtaining a smoothed Hazard and Survival Function estimate 3.5 Left Truncation "],["nonparametric-comparision-of-survival-distributions.html", "4 Nonparametric Comparision of Survival Distributions 4.1 Comparing Two Groups of Survival Times 4.2 Stratified Tests", " 4 Nonparametric Comparision of Survival Distributions 4.1 Comparing Two Groups of Survival Times two-side test vs one-sided test two-sample Students t-test rank-based Mann-Whitney test For survival analysis, nonparametric tests for \\(H_0:S_1(t)=S_0(t)\\) one-side: \\(H_A:S_1(t)&gt;S_0(t)\\) two-side: \\(H_A: S_1(t) \\neq S_0(t)\\) The relationship between \\(S_1(t)\\) and \\(S_0(t)\\) can be different with differnt \\(t\\) Lehman alternation: \\(H_A: S_1(t)=[S_0(t)]^\\psi\\), which is equivalent to \\(h_1(t)=\\psi h_0(t)\\). Then the one sided test would be \\(H_0:\\psi=1\\) vs \\(H_1: \\psi&lt;1\\). Construct a two-by-two table for each failure time \\(t_i\\), with \\(n_{0i}\\) and \\(n_{1i}\\) being the numbers at risk in group 1 and 2, and \\(d_{0i}\\) and \\(d_{1i}\\) being the number of failures in group 1 and 2. Based on hypergeometric distribution, we can have \\[p(d_{0i}|n_{0i},n_{1i},d_i)=\\frac{\\binom{n_{0i}}{d_{0i}}\\binom{n_{1i}}{d_{1i}}}{\\binom{n_i}{d_i}}\\] , where \\[\\binom{n}{d}=\\frac{n!}{d!(n-d)!}\\] The expected mean \\(e_{0i}\\) and variance \\(v_{0i}\\) can be given by \\[e_{0i}=E(d_{0i})=\\frac{n_{0i}d_i}{n_i}\\] \\[v_{0i}=var(d_{0i})=\\frac{n_{0i}n_{1i}d_i(n_i-d_i)}{n_i^2(n_i-1)}\\] We can sum the differences between the expected and observed values to get the test statistics \\(U_0\\) and its variance V_0 \\[U_0=\\sum_{i=1}^D(d_{0i}-e_{0i})=\\sum d_{0i} - \\sum e_{0i}\\] \\[V_0=var(U_0)=\\sum v_{0i}\\] Then the test statistic is \\[\\frac{U_0}{\\sqrt{V_0}} \\sim N(0,1)\\] , or \\[\\frac{U_0^2}{V_0} \\sim \\chi^2_1\\] This test is known as the log-rank test. library(survival) tt = c(6, 7, 10, 15, 19, 25) delta = c(1, 0, 1, 1, 0, 1) trt = c(0, 0, 1, 0, 1, 1) ## group 0 or 1 survdiff(Surv(tt, delta)~trt) ## Call: ## survdiff(formula = Surv(tt, delta) ~ trt) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## trt=0 3 2 1.08 0.776 1.27 ## trt=1 3 2 2.92 0.288 1.27 ## ## Chisq= 1.3 on 1 degrees of freedom, p= 0.3 A full table is The log-rank statistic is identical to Cochran-Mantel-Haenzel test in epidemiology, and may also be derived from the proportional hazards model. A generalization is to define a weighted log-rank test using weights \\(w_i\\) for D time points, \\[U_0(w)=\\sum w_i(d_{0i}-e_{0i})\\] \\[var(U_0)=\\sum w_i^2v_{0i}=V_0(w)\\] The most common way of setting weights is to sue the product-limit estimator from the combined sample \\[w_i=\\{\\hat{S}(t_i)\\}^\\rho\\] A log-rank test using these weights is called the Fleming-Harrington \\(G(\\rho)\\) test. If \\(\\rho=0\\), this test is equivalent to the log-rank test. If \\(\\rho=1\\), the test is called Prentice modification/Peto-Peto modification of the Gehan-Wilcoxon test, which place higher weight on the earlier survival differences. Back to Example 1.3 library(asaur) attach(pancreatic) Progression.d = as.Date(as.character(progression), &quot;%m/%d/%Y&quot;) OnStudy.d = as.Date(as.character(onstudy), &quot;%m/%d/%Y&quot;) Death.d = as.Date(as.character(death), &quot;%m/%d/%Y&quot;) progressionOnly = Progression.d - OnStudy.d overallSurvival = Death.d - OnStudy.d pfs = progressionOnly pfs[is.na(pfs)] = overallSurvival[is.na(pfs)] ### PFS: progression or death, whichever comes first pfs.month = pfs/30.5 plot(survfit(Surv(pfs.month) ~ stage), xlab=&quot;Time in months&quot;, ylab=&quot;Survival probability&quot;, col=c(&quot;blue&quot;, &quot;red&quot;), lwd=2) legend(&quot;topright&quot;, legend=c(&quot;Locally advanced&quot;, &quot;Metastatic&quot;), col=c(&quot;blue&quot;,&quot;red&quot;) , lwd=2) survdiff(Surv(pfs) ~ stage, rho=0) ### log-rank test ## Call: ## survdiff(formula = Surv(pfs) ~ stage, rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## stage=LA 8 8 12.3 1.49 2.25 ## stage=M 33 33 28.7 0.64 2.25 ## ## Chisq= 2.2 on 1 degrees of freedom, p= 0.1 survdiff(Surv(pfs) ~ stage, rho=1) ### Prentice modification ## Call: ## survdiff(formula = Surv(pfs) ~ stage, rho = 1) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## stage=LA 8 2.34 5.88 2.128 4.71 ## stage=M 33 18.76 15.22 0.822 4.71 ## ## Chisq= 4.7 on 1 degrees of freedom, p= 0.03 4.2 Stratified Tests If we need to compare two groups while adjusting for another covariate, we can include the other covarite (or multiple covarites) as regression terms for the hazard function (following chapters) use stratified log-rank test, if the covariate we are adjusting for is categorical with a small number of levels \\(G\\) The p value is 0.00299 for Prentice modification test, but 0.134 for log-rank test. From Figure, we can see the metastatic group shows an early survival advantage over the locally advanced group, but the survival curves converge after about 10 months. "],["regression-analysis-using-the-proportional-hazards-model.html", "5 Regression Analysis Using the Proportional Hazards Model 5.1 Covariates and nonparametric Survival Models 5.2 Comparing Two Survival Distributions Using a Partial Likelihood Function 5.3 Partial Likelihood Hypothesis Tests 5.4 The Partial Likelihood with Multiple Covariates 5.5 Handling of Tied Survival Times", " 5 Regression Analysis Using the Proportional Hazards Model 5.1 Covariates and nonparametric Survival Models As mentioned in last chapter, Lehmann alternative \\(S_1(t)=[S_0(t)]^{\\psi}\\) can be re-expressed in terms of the hazard functions, yielding the proportional hazards assumption, \\[h_1(t)=\\psi h_0(t)\\] Extend the model to include covariate information in a vector \\(z\\) as follows: \\[\\psi = e^{z\\beta}\\] We can model this by partial likelihood, which was initially developed by D.R. Cox (1972), so it is often referred to as the Cox proportional hazards model 5.2 Comparing Two Survival Distributions Using a Partial Likelihood Function Partial likelihood does not require the assumptions about the survival distributions (parametric survial distributions), which is required by the standard likelihood method in section 2.6. Under the proportional hazards model, assuming one single binary covarite \\(z\\), the hazard function for the i-th subject at the j-th failure time is \\[h_i(t_j)=h_0(t_j)\\psi_i\\text{, and }\\psi_i=e^{z_i\\beta}\\] , where i is the index of subject, \\(i=1,...,n\\), with n being the sample size j is the index of failure time, \\(j=1,...,D\\), with D being the number of failure times \\(z_i\\) is the covariate (the simple case: \\(z_i=1\\) when i-th subject in experimental group, or \\(z_i=0\\) when i-th subject in the control group) \\(\\beta\\) is the effect size for the covariate Consider the first failure time \\(t_1\\) and we have one failure at \\(t_1\\), the probability that Patient \\(i\\) is the failure is \\[p_1=p_1(\\text{subject i fails})=\\frac{h_i(t_1)}{\\sum_{k \\in R_1}h_k(t_1)}=\\frac{h_0(t_1)\\psi_i}{\\sum_{k\\in R_1}h_0(t_1)\\psi_k}=\\frac{\\psi_i}{\\sum_{k\\in R_i}\\psi_k}\\] , where \\(R_1\\) is the set of subjects at risks at time \\(t_1\\) (i.e., all the subjects before the first failure) A key fact here is that the baseline hazard \\(h_0(t_1)\\) cancels out of the numerator and denominator. We can repeat this calculation for the second failure time \\(t_2\\) among the set \\(R_2\\), which is the \\(R_1\\) excluding the failure one and also the censored ones between \\(t_1\\) and \\(t_2\\). So the partial likelihood is \\[L(\\psi)=p_1p_2...p_D\\] Consider the synethtic data in section 4.1, library(survival) tt = c(6, 7, 10, 15, 19, 25) delta = c(1, 0, 1, 1, 0, 1) trt = c(0, 0, 1, 0, 1, 1) ## group 0 or 1 summary(survfit(Surv(tt,delta)~1)) ## Call: survfit(formula = Surv(tt, delta) ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 6 6 1 0.833 0.152 0.583 1 ## 10 4 1 0.625 0.213 0.320 1 ## 15 3 1 0.417 0.222 0.147 1 ## 25 1 1 0.000 NaN NA NA At the first failure time, \\(t_1=6\\) and \\(R_1\\) included six subjects at risk. Considering subject 1, 2, and 4 are in the control group (\\(z=0\\)) and subject 3, 5, and 6 are in the experiment group (\\(z=1\\)), we can have \\[\\psi_1=\\psi_2=\\psi_4=e^{0*\\beta}=1\\] , and \\[\\psi_3=\\psi_5=\\psi_6=e^{1*\\beta}=\\psi\\] We observed the first subject (subject 1) failed at time 6, then \\[p_1=\\frac{1}{3\\psi+3}\\] Similarly, at the second failure time (\\(t_2=10\\)), the \\(R_2\\) has 4 subjects at risk, excluding subject 1 failed in the first failure time and subject 2 censored at time 7. Subject 3 failed, and its probability is \\[p_2=\\frac{\\psi}{3\\psi+1}\\]. Similarly, we can get \\[p_3=\\frac{1}{2\\psi+1}\\text{ and }p_4=1\\] So the partial likelihood is \\[L(\\psi)=p_1p_2p_3p_4=\\frac{\\psi}{(3\\psi+3)(3\\psi+1)(2\\psi+1)}\\] Plug in \\(\\psi=e^{\\beta}\\) and consider log partial likelihood, we can get \\[l(\\beta)=\\beta-ln(e^\\beta+3)-ln(3e^\\beta+1)-ln(2e^\\beta+1)\\] The maximum partial likelihood estimate is \\[\\hat{\\beta}_{MPLE}=arg max_\\psi l(\\beta)\\] , which is independent of the baseline hazard function \\(h_0(t)\\) plsimple = function(beta){ psi = exp(beta) l = beta-log(3*psi+3)-log(3*psi+1)-log(2*psi+1) return(l) } ## fnscale=1 means the optim will find the maximum result = optim(par=0, fn=plsimple, method=&quot;L-BFGS-B&quot;, control=list(fnscale=-1), lower=-3, upper=1) ### MPLE result$par ## [1] -1.326129 plsimple(result$par) ## [1] -3.671981 plsimple(0) ## [1] -4.276666 betas = seq(-4,1.2,length.out=100) plot(betas, plsimple(betas), type=&quot;l&quot;, xlab=&quot;beta&quot;, ylab=&quot;log partial likelihood&quot;) points(result$par, plsimple(result$par), col=&quot;blue&quot;, pch=16) points(0, plsimple(0), col=&quot;red&quot;, pch=16) 5.3 Partial Likelihood Hypothesis Tests Three forms of test of \\(H_0: \\beta=0\\), which they yield similar results. the score function, the first derivative of the log likelihood, \\(S(\\beta)=l&#39;(\\beta)\\) the information function, minus the derivative of the score function, \\(I(\\beta)=-S&#39;(\\beta)=-l&#39;&#39;(\\beta)\\), where \\(l&#39;&#39;(\\beta)\\) is also called Hessian Plug in parameter estimate \\(\\hat{\\beta}\\) into information function, we can get observed information 5.3.1 The Wald Test we have the MPLE \\(\\hat{\\beta}\\), we can get the observed information \\[I(\\hat{\\beta})=-l&#39;&#39;(\\hat{\\beta})\\] The variance of \\(\\hat{\\beta}\\) is approximately \\[var(\\hat{\\beta})=1/I(\\hat{\\beta})\\text{ or }se(\\hat{\\beta})=1/\\sqrt{I(\\hat{\\beta})}\\] The test can be done by \\[\\hat{\\beta}/se(\\hat{\\beta}) \\sim N(0,1)\\text{ or } \\hat{\\beta}^2 \\sim \\chi^2_{df=1}\\] 5.3.2 The Score Test Evaluate the score and inofrmation at the null hpyothesis value of \\(\\beta=0\\) \\[S(\\beta=0)/\\sqrt{I(\\beta=0)} \\sim N(0,1)\\] The score test is equivalent to the log-rank test. It can be carried out without finding the maximum likelihood estimate \\(\\hat{\\beta}_{MPLE}\\) 5.3.3 The likelihood Ratio Test \\[2[l(\\beta=\\hat{\\beta})-l(\\beta=0)] \\sim \\chi^2_1\\] The key advantage of this test is it is invariant to monotonic transofrmations of \\(\\beta\\) Continue with the example above result.cox = coxph(Surv(tt, delta)~trt) summary(result.cox) ## provides the results of these three tests ## Call: ## coxph(formula = Surv(tt, delta) ~ trt) ## ## n= 6, number of events= 4 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## trt -1.3261 0.2655 1.2509 -1.06 0.289 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## trt 0.2655 3.766 0.02287 3.082 ## ## Concordance= 0.7 (se = 0.116 ) ## Likelihood ratio test= 1.21 on 1 df, p=0.3 ## Wald test = 1.12 on 1 df, p=0.3 ## Score (logrank) test = 1.27 on 1 df, p=0.3 ## score test #install.packages(&quot;numDeriv&quot;) library(numDeriv) ### calculate the S(beta=0) beta0 = grad(func=plsimple, x=0) ### calculate the information var0 = -hessian(func=plsimple, x=0) beta0^2/var0 ## [,1] ## [1,] 1.273694 pchisq(beta0^2/var0, df=1, lower.tail=F) ## [,1] ## [1,] 0.2590748 ### Wald test ### calculate the information function at beta_MPLE -hessian(func=plsimple, x=result$par) ## [,1] ## [1,] 0.639117 ### calculate the variance 1/(-hessian(func=plsimple, x=result$par)) ## [,1] ## [1,] 1.564659 ### calculate the test statistics result$par^2*(-hessian(func=plsimple, x=result$par)) ## [,1] ## [1,] 1.123963 ### likelihood ratio statistics l_full = plsimple(result$par) l_reduce = plsimple(0) 2*(l_full-l_reduce) ## [1] 1.209369 Two additional output are often useful. The first one is statistic “r-squared” (or Pseudo \\(R^2\\)), \\[R^2_{Cox\\&amp;Snell}=1-(\\frac{L(0)}{L(\\hat{\\beta})})^{2/n}=1-e^{2/n[l(0)-l(\\hat{\\beta})]}\\] \\[R^2_{Nagelkerke}=\\frac{R^2_{Cox\\&amp;Snell}}{1-e^{2/n*l(0)}}\\] , where \\(l()\\) is the log-partial likelihood function and \\(L()\\) is the partial likelihood function ### r-squared (which is not included in the current version of survival R package: see https://stackoverflow.com/questions/56071385/r-squared-missing-from-output-of-coxph) CSR2 = 1-exp((2/length(tt))*(l_reduce-l_full)) R2max = 1-exp((2/length(tt))*(l_reduce)) NKR2 = CSR2/R2max CSR2 ## [1] 0.1825467 R2max ## [1] 0.7596251 NKR2 ## [1] 0.2403116 The second one is Concordance index, or c-index, or Harrell’s C, a measure of the predictive discrimination of a covariate. 5.4 The Partial Likelihood with Multiple Covariates 5.5 Handling of Tied Survival Times "],["model-selection-and-interpretation.html", "6 Model Selection and Interpretation 6.1 Covariate Adjustment 6.2 Categorical and Continuous Covariates 6.3 Hypothesis Testing for Nested Models 6.4 The Akaike Information Criterion for Comparing Non-nested Models 6.5 Including Smooth Estimates of Continuous Covariates in a Survival Model", " 6 Model Selection and Interpretation 6.1 Covariate Adjustment 6.2 Categorical and Continuous Covariates 6.3 Hypothesis Testing for Nested Models 6.4 The Akaike Information Criterion for Comparing Non-nested Models 6.5 Including Smooth Estimates of Continuous Covariates in a Survival Model "]]
